#!/bin/bash
# Project Initialization Script
# CNN-based Disease Detection via GAN-based Data Augmentation

echo "=================================================="
echo "  GAN-based Disease Detection Project Setup"
echo "=================================================="
echo ""

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored messages
print_success() {
    echo -e "${GREEN}âœ“ $1${NC}"
}

print_error() {
    echo -e "${RED}âœ— $1${NC}"
}

print_info() {
    echo -e "${YELLOW}â†’ $1${NC}"
}

# Check if conda is installed
if ! command -v conda &> /dev/null; then
    print_error "Conda not found. Please install Miniconda/Anaconda first."
    exit 1
fi
print_success "Conda found"

# Create project directory structure
print_info "Creating project directory structure..."

mkdir -p data/{raw,processed,splits,synthetic}/{dermatofibroma,nevus}
mkdir -p data/splits/{train,val,test}
mkdir -p data/synthetic/{raw,filtered,reviewed,curated}
mkdir -p data/metadata
mkdir -p models
mkdir -p training
mkdir -p evaluation
mkdir -p preprocessing
mkdir -p utils
mkdir -p notebooks
mkdir -p results/{figures,tables,checkpoints}
mkdir -p logs

print_success "Directory structure created"

# Create requirements.txt
print_info "Creating requirements.txt..."

cat > requirements.txt << 'EOF'
# Core Deep Learning Framework
torch==2.0.1
torchvision==0.15.2
torchaudio==2.0.2

# Data Processing and Manipulation
numpy==1.24.3
pandas==2.0.3
pillow==10.0.0
opencv-python==4.8.0.74
scikit-image==0.21.0
h5py==3.9.0

# Machine Learning and Statistics
scikit-learn==1.3.0
scipy==1.11.1
statsmodels==0.14.0

# GAN Evaluation Metrics
pytorch-fid==0.3.0
lpips==0.1.4
clean-fid==0.1.35

# Visualization and Plotting
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.16.1

# Experiment Tracking and Logging
tensorboard==2.13.0
wandb==0.15.8

# Progress Bars and Utilities
tqdm==4.65.0
pyyaml==6.0.1

# Jupyter and Interactive Development
jupyter==1.0.0
ipykernel==6.25.0
notebook==7.0.2

# Testing and Quality
pytest==7.4.0
black==23.7.0
flake8==6.1.0

# Additional Utilities
kaggle==1.5.16
gdown==4.7.1
EOF

print_success "requirements.txt created"

# Create .gitignore
print_info "Creating .gitignore..."

cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Jupyter Notebook
.ipynb_checkpoints
*.ipynb

# PyTorch
*.pth
*.pt
checkpoints/

# Data
data/raw/
data/processed/
data/splits/
data/synthetic/
*.csv
*.h5
*.hdf5

# Logs
logs/
*.log
runs/

# IDEs
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Project specific
results/
*.zip
*.tar.gz
*.pkl
*.pickle
EOF

print_success ".gitignore created"

# Create README.md
print_info "Creating README.md..."

cat > README.md << 'EOF'
# CNN-based Disease Detection via GAN-based Data Augmentation

A deep learning project for rare disease detection (Dermatofibroma) using WGAN-GP for synthetic data generation to address extreme class imbalance.

## ðŸ“Š Project Overview

- **Dataset**: ISIC 2020 Challenge (Dermatofibroma vs Melanocytic Nevus)
- **Imbalance Ratio**: 56.7:1 (300 rare vs 17,000 common samples)
- **Method**: WGAN-GP for synthetic data generation
- **Goal**: Improve F1-score from 0.51 to 0.76 (+49% improvement)

## ðŸš€ Quick Start

### 1. Environment Setup

```bash
# Create conda environment
conda create -n gan-disease python=3.10 -y
conda activate gan-disease

# Install PyTorch with CUDA
conda install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia -y

# Install other dependencies
pip install -r requirements.txt
```

### 2. Dataset Download

```bash
# Option A: Using Kaggle API (Recommended)
pip install kaggle
kaggle datasets download -d sumaiyabinteshahid/isic-challenge-dataset-2020
unzip isic-challenge-dataset-2020.zip -d data/raw/

# Option B: Manual download from https://challenge2020.isic-archive.com/
```

### 3. Data Preprocessing

```bash
python preprocessing/prepare_dataset.py
```

### 4. Train WGAN-GP

```bash
python training/train_gan.py --config configs/wgan_gp_config.yaml
```

### 5. Generate and Curate Synthetic Images

```bash
python preprocessing/generate_synthetic.py
python preprocessing/curate_images.py
```

### 6. Train Classifier

```bash
# Baseline
python training/train_classifier.py --mode baseline

# With GAN augmentation
python training/train_classifier.py --mode gan_augmented
```

### 7. Evaluate Results

```bash
python evaluation/evaluate_all.py
```

## ðŸ“ Project Structure

```
â”œâ”€â”€ data/                      # Data directory
â”‚   â”œâ”€â”€ raw/                   # Original ISIC 2020 dataset
â”‚   â”œâ”€â”€ processed/             # Preprocessed images (256Ã—256)
â”‚   â”œâ”€â”€ splits/                # Train/val/test splits
â”‚   â””â”€â”€ synthetic/             # Generated synthetic images
â”œâ”€â”€ models/                    # Model architectures
â”‚   â”œâ”€â”€ wgan_gp.py            # WGAN-GP implementation
â”‚   â”œâ”€â”€ generator.py          # Generator network
â”‚   â”œâ”€â”€ discriminator.py      # Discriminator/Critic network
â”‚   â””â”€â”€ classifiers.py        # CNN classifiers
â”œâ”€â”€ training/                  # Training scripts
â”‚   â”œâ”€â”€ train_gan.py          # GAN training loop
â”‚   â””â”€â”€ train_classifier.py   # CNN training loop
â”œâ”€â”€ evaluation/                # Evaluation scripts
â”‚   â”œâ”€â”€ metrics.py            # Evaluation metrics
â”‚   â””â”€â”€ statistical_tests.py  # Statistical validation
â”œâ”€â”€ preprocessing/             # Data preprocessing
â”‚   â”œâ”€â”€ prepare_dataset.py    # Dataset preparation
â”‚   â”œâ”€â”€ data_loader.py        # Data loading utilities
â”‚   â””â”€â”€ curation.py           # Synthetic image curation
â”œâ”€â”€ utils/                     # Utility functions
â”‚   â”œâ”€â”€ visualization.py      # Plotting and visualization
â”‚   â””â”€â”€ logger.py             # Logging utilities
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”œâ”€â”€ results/                   # Experimental results
â””â”€â”€ configs/                   # Configuration files
```

## ðŸŽ¯ Key Results

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| Baseline (Imbalanced) | 82.4% | 64.3% | 28.1% | 0.51 |
| Traditional Aug | 84.9% | 71.2% | 42.0% | 0.526 |
| **GAN-GP Aug (Ours)** | **88.5%** | **78.3%** | **59.5%** | **0.76** |

**Improvement**: +49% F1-score compared to baseline

## ðŸ“ Citation

```bibtex
@misc{gan-disease-detection-2024,
  title={CNN-based Disease Detection via GAN-based Data Augmentation},
  author={Aditya Pratap Singh, Nandini Pandey, Priya Rani},
  year={2024},
  institution={Dr. B.C. Roy Engineering College}
}
```

## ðŸ“„ License

This project uses the ISIC 2020 dataset, which is licensed under Creative Commons Attribution-Non Commercial 4.0 International License.

## ðŸ‘¥ Authors

- Aditya Pratap Singh (12000122063)
- Nandini Pandey (12000122069)
- Priya Rani (12031522016)

**Supervisor**: Prof. Amitabha Mondal

## ðŸ™ Acknowledgments

- International Skin Imaging Collaboration (ISIC) for the dataset
- PyTorch team for the deep learning framework
- Research papers that inspired this work

---

**Note**: This is a research project for academic purposes. The synthetic images should not be used for clinical diagnosis without proper validation.
EOF

print_success "README.md created"

# Create configuration file
print_info "Creating configuration file..."

mkdir -p configs

cat > configs/wgan_gp_config.yaml << 'EOF'
# WGAN-GP Configuration

# Data
data:
  dataset_path: "data/splits/train/dermatofibroma"
  image_size: 256
  channels: 3
  batch_size: 16
  num_workers: 4

# Model Architecture
model:
  latent_dim: 100
  generator_features: 16
  discriminator_features: 16

# Training
training:
  num_epochs: 240
  total_iterations: 50000
  learning_rate_generator: 0.0001
  learning_rate_critic: 0.0001
  beta1: 0.5
  beta2: 0.999
  lambda_gp: 10
  critic_iterations: 5

# Checkpointing
checkpoint:
  save_interval: 5000
  sample_interval: 1000
  checkpoint_dir: "results/checkpoints/wgan_gp"

# Logging
logging:
  tensorboard: true
  log_dir: "logs/wgan_gp"
  wandb: false
  wandb_project: "gan-disease-detection"

# Device
device:
  use_cuda: true
  gpu_id: 0

# Random Seed
seed: 42
EOF

cat > configs/classifier_config.yaml << 'EOF'
# CNN Classifier Configuration

# Data
data:
  train_path: "data/splits/train"
  val_path: "data/splits/val"
  test_path: "data/splits/test"
  synthetic_path: "data/synthetic/curated"
  image_size: 256
  batch_size: 32
  num_workers: 4

# Model
model:
  architecture: "resnet50"  # Options: resnet50, densenet121, efficientnet_b0
  pretrained: true
  num_classes: 2

# Training
training:
  num_epochs: 50
  learning_rate: 0.0001
  weight_decay: 0.0001
  optimizer: "Adam"
  early_stopping_patience: 10
  class_weights: [1.0, 56.7]  # [nevus, dermatofibroma]

# Data Augmentation
augmentation:
  use_traditional: true
  use_gan: false  # Set to true for GAN-augmented training
  
  traditional:
    rotation_degrees: 30
    horizontal_flip: 0.5
    vertical_flip: 0.5
    color_jitter: 0.2

# Evaluation
evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1_score
    - roc_auc
  
  save_confusion_matrix: true
  save_roc_curve: true

# Checkpointing
checkpoint:
  save_best_only: true
  monitor: "val_f1"
  checkpoint_dir: "results/checkpoints/classifier"

# Logging
logging:
  tensorboard: true
  log_dir: "logs/classifier"
  wandb: false

# Device
device:
  use_cuda: true
  gpu_id: 0

# Random Seed
seed: 42
EOF

print_success "Configuration files created"

# Create main.py
print_info "Creating main.py..."

cat > main.py << 'EOF'
"""
Main execution script for GAN-based Disease Detection project.
"""

import argparse
import sys
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='GAN-based Disease Detection')
    parser.add_argument('--mode', type=str, required=True,
                      choices=['preprocess', 'train_gan', 'generate', 
                              'curate', 'train_classifier', 'evaluate'],
                      help='Execution mode')
    parser.add_argument('--config', type=str, default=None,
                      help='Path to configuration file')
    
    args = parser.parse_args()
    
    if args.mode == 'preprocess':
        print("Running data preprocessing...")
        # Import and run preprocessing
        
    elif args.mode == 'train_gan':
        print("Training WGAN-GP...")
        # Import and run GAN training
        
    elif args.mode == 'generate':
        print("Generating synthetic images...")
        # Import and run generation
        
    elif args.mode == 'curate':
        print("Curating synthetic images...")
        # Import and run curation
        
    elif args.mode == 'train_classifier':
        print("Training CNN classifier...")
        # Import and run classifier training
        
    elif args.mode == 'evaluate':
        print("Evaluating models...")
        # Import and run evaluation
    
    print(f"âœ“ {args.mode} completed successfully!")

if __name__ == '__main__':
    main()
EOF

print_success "main.py created"

# Create environment setup script
print_info "Creating environment setup script..."

cat > setup_environment.sh << 'EOF'
#!/bin/bash
# Environment Setup Script

echo "Setting up GAN Disease Detection environment..."

# Check if conda is available
if ! command -v conda &> /dev/null; then
    echo "Error: Conda not found. Please install Miniconda/Anaconda."
    exit 1
fi

# Create environment
echo "Creating conda environment..."
conda create -n gan-disease python=3.10 -y

# Activate environment
echo "Activating environment..."
source $(conda info --base)/etc/profile.d/conda.sh
conda activate gan-disease

# Install PyTorch with CUDA
echo "Installing PyTorch with CUDA support..."
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# Install other dependencies
echo "Installing other dependencies..."
pip install -r requirements.txt

# Verify installation
echo "Verifying installation..."
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

echo ""
echo "âœ“ Environment setup complete!"
echo "To activate the environment, run: conda activate gan-disease"
EOF

chmod +x setup_environment.sh
print_success "setup_environment.sh created"

# Create dataset download script
print_info "Creating dataset download script..."

cat > download_dataset.sh << 'EOF'
#!/bin/bash
# Dataset Download Script

echo "Downloading ISIC 2020 Dataset..."

# Check if kaggle CLI is installed
if ! command -v kaggle &> /dev/null; then
    echo "Installing Kaggle CLI..."
    pip install kaggle
fi

# Check if kaggle credentials exist
if [ ! -f ~/.kaggle/kaggle.json ]; then
    echo "Error: Kaggle credentials not found."
    echo "Please download kaggle.json from https://www.kaggle.com/settings"
    echo "and place it in ~/.kaggle/"
    exit 1
fi

# Set permissions
chmod 600 ~/.kaggle/kaggle.json

# Download dataset
echo "Downloading dataset (this may take a while)..."
kaggle datasets download -d sumaiyabinteshahid/isic-challenge-dataset-2020

# Unzip dataset
echo "Extracting dataset..."
unzip -q isic-challenge-dataset-2020.zip -d data/raw/

# Clean up zip file
rm isic-challenge-dataset-2020.zip

echo "âœ“ Dataset download complete!"
echo "Dataset location: data/raw/"
EOF

chmod +x download_dataset.sh
print_success "download_dataset.sh created"

# Create verification script
print_info "Creating verification script..."

cat > verify_setup.py << 'EOF'
"""
Verification script to check if all dependencies and data are ready.
"""

import sys
import os
from pathlib import Path

def check_python_version():
    """Check Python version"""
    version = sys.version_info
    print(f"Python version: {version.major}.{version.minor}.{version.micro}")
    if version.major == 3 and version.minor >= 8:
        print("âœ“ Python version OK")
        return True
    else:
        print("âœ— Python version should be 3.8 or higher")
        return False

def check_dependencies():
    """Check if all required packages are installed"""
    required_packages = [
        'torch',
        'torchvision',
        'numpy',
        'pandas',
        'cv2',
        'sklearn',
        'matplotlib',
        'lpips',
        'pytorch_fid'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            if package == 'cv2':
                __import__('cv2')
            elif package == 'sklearn':
                __import__('sklearn')
            else:
                __import__(package)
            print(f"âœ“ {package}")
        except ImportError:
            print(f"âœ— {package} not found")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nMissing packages: {', '.join(missing_packages)}")
        print("Run: pip install -r requirements.txt")
        return False
    
    return True

def check_cuda():
    """Check CUDA availability"""
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ“ CUDA available")
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            return True
        else:
            print("âš  CUDA not available. Training will be slow on CPU.")
            return False
    except:
        return False

def check_directory_structure():
    """Check if directory structure is correct"""
    required_dirs = [
        'data/raw',
        'data/processed',
        'data/splits',
        'data/synthetic',
        'models',
        'training',
        'evaluation',
        'preprocessing',
        'utils',
        'notebooks',
        'results',
        'configs'
    ]
    
    all_exist = True
    for dir_path in required_dirs:
        if Path(dir_path).exists():
            print(f"âœ“ {dir_path}")
        else:
            print(f"âœ— {dir_path} not found")
            all_exist = False
    
    return all_exist

def check_dataset():
    """Check if dataset is downloaded"""
    data_path = Path('data/raw')
    
    if not data_path.exists():
        print("âœ— Dataset directory not found")
        return False
    
    # Check for some expected files
    files = list(data_path.rglob('*.jpg')) + list(data_path.rglob('*.jpeg'))
    
    if len(files) > 0:
        print(f"âœ“ Found {len(files)} images in dataset")
        return True
    else:
        print("âš  No images found in data/raw/")
        print("  Run: ./download_dataset.sh")
        return False

def main():
    print("=" * 60)
    print("  GAN Disease Detection - Setup Verification")
    print("=" * 60)
    print()
    
    results = []
    
    print("Checking Python version...")
    results.append(check_python_version())
    print()
    
    print("Checking dependencies...")
    results.append(check_dependencies())
    print()
    
    print("Checking CUDA...")
    results.append(check_cuda())
    print()
    
    print("Checking directory structure...")
    results.append(check_directory_structure())
    print()
    
    print("Checking dataset...")
    results.append(check_dataset())
    print()
    
    print("=" * 60)
    if all(results):
        print("âœ“ All checks passed! Ready to start development.")
    else:
        print("âš  Some checks failed. Please resolve issues before proceeding.")
    print("=" * 60)

if __name__ == '__main__':
    main()
EOF

print_success "verify_setup.py created"

# Final message
echo ""
echo "=================================================="
print_success "Project initialization complete!"
echo "=================================================="
echo ""
echo "Next steps:"
echo "  1. Run: ./setup_environment.sh (to create Python environment)"
echo "  2. Run: ./download_dataset.sh (to download ISIC 2020 dataset)"
echo "  3. Run: python verify_setup.py (to verify everything is ready)"
echo "  4. Start development following CLAUDE_CODE_PROMPT.md"
echo ""
echo "For detailed instructions, see:"
echo "  - CLAUDE_CODE_PROMPT.md (complete implementation guide)"
echo "  - DATASET_AND_RESOURCES.md (dataset info and resources)"
echo "  - README.md (project overview)"
echo ""
print_info "Happy coding! ðŸš€"
